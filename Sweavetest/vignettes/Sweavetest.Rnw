\documentclass[12pt]{article}
\usepackage{Sweave}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{longtable}

\usepackage[authoryear,round,comma]{natbib}
\bibliographystyle{apa}

%\VignetteIndexEntry{About the Sweavetest package}

% The next line is needed for inverse search...
\SweaveOpts{echo=TRUE,results=verbatim} % to show R code and results
\SweaveOpts{concordance=TRUE, keep.source=TRUE,echo=FALSE,results=tex}
<<>>=
options(width=60)
set.seed(18)
install.packages("~/git/Sweavetest/Sweavetest",type="source",repos=NULL)
library(Sweavetest)
randomize <- TRUE
testversion <- 1
newCommands(thesection=FALSE)
@

\newcommand{\pkg}[1]{{\fontseries{b}\selectfont #1}}

% These commands affect the display of the test samples
\newcommand{\Correct}{*}
\renewcommand{\labelenumii}{(\Alph{enumii})}

\title{The \pkg{Sweavetest} Package\footnote{This 
vignette  was built using \pkg{Sweavetest} version \Sexpr{packageDescription("Sweavetest")$Version}}.}%$

\author{Duncan Murdoch \and Adam Rahman}

\begin{document}

\maketitle

\abstract{

Multiple choice tests are used frequently to assess student
performance in large classes. The \pkg{Sweavetest} package
contains support functions to format, grade and analyze multiple
choice tests.  Answers are randomized during formatting,
and after grading a report is automatically generated, showing descriptive
statistics, measuring test reliability, and presenting item analysis.
Errors in answer keys and poorly worded questions are easily
recognized through the information generated. Several plots are also
created to visually show the instructor how well the students are
performing, and to help identify possible problem questions
and answer patterns that suggest cheating.
}


\tableofcontents

\section{Introduction}

The \pkg{Sweavetest} package is designed to support administration of multiple choice
tests.  It was written at the University of Western Ontario (UWO), and some aspects of the code
reflect the way we administer tests, but efforts have been made to make it more generally
useful.

There are four main parts to the package, described in the following sections.  First, a
number of functions are given to help an instructor prepare a test using Sweave \citep{Leisch2002}.
These are described in section \ref{sec:preparing}.  At UWO, the students use Scantron 
mark-sense sheets to record answers on multiple-choice tests; section \ref{sec:reading}
describes how these are handled in \pkg{Sweavetest}, producing grades for each student
and merging them into existing class lists.

An important part of testing is to analyze the results of a test.  This can detect badly
worded questions, errors in answer keys, and can otherwise give information about the
quality of the questions and of the test as a whole.  Section \ref{sec:analysis} describes
this aspect of \pkg{Sweavetest}.  It may also be possible to use the aggregate results
to detect cheating by individual students; section \ref{sec:cheating} describes how this is done.
This paper finishes with a short discussion of plans for changes to \pkg{Sweavetest}.


\section{Preparing a Test}
\label{sec:preparing}

\pkg{Sweavetest} was originally designed to support writing multiple choice tests in Sweave
and \LaTeX.  There are functions that output introductory material at the start of the
test, as well as functions that format and randomize the answers.  These functions also
record the results of the randomization so that an answer key can be produced automatically
at the end.

Using the test preparation functions requires knowledge of Sweave and \LaTeX.  Instructors
who want to use other document preparation systems to produce the test can skip this section,
but some of the work done here automatically will have to be done manually later.

\subsection{Writing questions}
\label{sec:questions}

The functions to randomize answers all work on the responses to questions; the question text
itself is not important, but it is assumed that all questions have multiple choice responses.  See below for how to handle tests where some questions are in other formats.

\pkg{Sweavetest} supports four separate versions of each test; questions
may contain from 2 to 5 answers.  The preamble to the test randomizes the test version
numbers to be 4  numbers from 100 to 999 in sorted order.  Version 1 of the test has the smallest 
test version number, version 4 the largest.

The supplied functions do the work of formatting the answers.   For example,
\begin{verbatim}
 \item Let $U \sim \Unif(0,1)$.  Set $X$ equal to 0 if 
 $U > 0.3$, equal to 1 otherwise.  The distribution of 
 $X$ is
 <<>>=
 enumerate("Binomial$(n=1, p=0.7)$", 
      "Binomial$(n=1, p=0.3)$",
      "Poisson$(\\mu = 0.3)$",
      "Poisson$(\\mu = 0.7)$",
      "Exponential$(\\lambda=0.3)$",
      Correct=2)
 @
 \end{verbatim}
The \verb!enumerate()! function produces
an enumeration environment in \LaTeX.  The answers are displayed in a random order.  Earlier in the text, we've used
\begin{verbatim}
 \newcommand{\Correct}{*}
 \renewcommand{\labelenumii}{(\Alph{enumii})}
\end{verbatim}
so that the correct answer is marked with an asterisk, and the answers are labelled with 
capital letters.  (We wouldn't label the correct answer on the version of the test we give to 
students!)  We also used \verb!\SweaveOpts{echo=FALSE,results=tex}! earlier to set the defaults
to output \LaTeX\ code without echoing the input.   With this preparation, the Sweave code listed
above will be processed and displayed as
\begin{enumerate}
\item Let $U \sim \Unif(0,1)$.  Set $X$ equal to 0 if $U > 0.3$, equal to 1 otherwise.  The
distribution of $X$ is
<<>>=
enumerate("$\\Binom(n=1, p=0.7)$", 
      "$\\Binom(n=1, p=0.3)$",
      "$\\Poisson(\\mu = 0.3)$",
      "$\\Poisson(\\mu = 0.7)$",
      "$\\Exp(\\lambda=0.3)$",
      Correct=2)
@
\end{enumerate}

Besides \verb!enumerate()!, several other functions produce randomized answer lists.  The
\verb!horiz()! function formats them horizontally on a single line.  The \verb!items()! function
is like \verb!enumerate()!, but only the \LaTeX\ \verb!\items! part of the enumeration is output:
it is assumed that the test writer has already entered \verb!\begin{enumerate}! or equivalent.
The \verb!things()! function just randomizes the items it is given; the test writer needs to
enter the \verb!\item! values if necessary.

The \pkg{Sweavetest} package also has special support for questions
involving R code. \verb!Renumerate()!, \verb!Rhoriz()! and 
\verb!Ritems()! format text to look like R input expressions. The 
\verb!resultenumerate()! and \verb!resultitems()! function evaluate the
arguments and format the output as if it was printed as output by R,
unless they are protected by being wrapped in the \verb!msg()! function.
For example, the input
\begin{verbatim}
 \item Which of the following gives the best approximation to 
 $\int_0^5 (x + 4) \cos(x) dx$?
 <<>>=
 Renumerate(Correct=2,
 "x <- runif(1000, 0, 5); mean((x + 4)*cos(x))",
 "x <- runif(1000, 0, 5); mean(5*(x + 4)*cos(x))",
 "x <- runif(1000, 0, 5); mean((x + 4)*cos(x)/5)",
 "x <- rcos(1000, 0, 5); mean(x + 4)",
 "42")
 @
\end{verbatim}
produces
\begin{enumerate}
\setcounter{enumi}{1}
\item Which of the following gives the best approximation to \\
 $\int_0^5 (x + 4) \cos(x) dx$?
<<>>=
 Renumerate(Correct=2,
 "x <- runif(1000, 0, 5); mean((x + 4)*cos(x))",
 "x <- runif(1000, 0, 5); mean(5*(x + 4)*cos(x))",
 "x <- runif(1000, 0, 5); mean((x + 4)*cos(x)/5)",
 "x <- rcos(1000, 0, 5); mean(x + 4)",
 "42")
@ 
\end{enumerate}
and 
\begin{verbatim}
 \item What will the following code produce?
 <<results=hide,echo=TRUE>>=
 x <- rep(c(TRUE, FALSE), 2)
 y <- rep(c(TRUE, FALSE), each=2)
 x & !y
 @
 <<>>=
 resultenumerate(Correct=1, KeepLast=1, x & !y, x & y,
 x | !y, x | y, msg("None of the above"))
 @
\end{verbatim}

\begin{enumerate}
\setcounter{enumi}{2}
\item What will the following code produce?
<<results=hide,echo=TRUE>>=
x <- rep(c(TRUE, FALSE), 2)
y <- rep(c(TRUE, FALSE), each=2)
x & !y
@
<<>>=
resultenumerate(Correct=1, KeepLast=1,
x & !y,
x & y,
x | !y,
x | y,
msg("None of the above"))
@
\end{enumerate}

If some question numbers are not used for multiple choice questions, then the test author can
manipulate the internal variables as follows.

[Adam, I need to have the current format of the internals before I can write this.]

\subsection{Introductory material at start of test}

Several functions and variables are included that are intended to be run or set 
at the start of a test.  These customize the behaviour of \pkg{Sweavetest}, and 
provide some common definitions that may be helpful in producing a test.

\begin{table}
\caption{Current math mode definitions provided by \texttt{newCommands()}.\label{tab:commands}}
\begin{center}
\begin{tabular}{lc}
\hline
Macro & Sample \\
\hline
<<>>=
n <- names(formals(newCommands))
for(i in n) {
  if (!(i %in% c("Marks", "thesection", "lowtilde")))
    cat("\\verb!\\", i, "! & $ \\", i, " $ \\\\ \n", sep="")
}
@
\hline
\end{tabular}
\end{center}
\end{table}
The \verb!newCommands()! function defines a number of \LaTeX\ macros that may be
convenient to use in the test.  Current definitions of simple macros
to use in mathematical expressions are shown in Table \ref{tab:commands}.  The function
also optionally defines \verb!\Marks!, which displays marks for a question in the right
margin, \verb!\thesection!, which suppresses section numbering, and \verb!\lowtilde!,
which draws a tilde under a math symbol, similar to the convention used on a blackboard
to indicate a vector.  By default, all macros are defined, but any of them
may be suppressed by passing \verb!FALSE! to the corresponding argument to 
\verb!newCommands()!.  For example,
this document uses \verb!newCommands(thesection = FALSE)!, because section headings
should not be overridden.

The \verb!marklist()! function outputs a box, suitable for recording marks on the
first page of a test.  For example,
<<marklist, echo=TRUE, eval=FALSE>>=
marklist(marks=rep(5,4), names=1:4)
@
produces the text and box shown in Figure \ref{fig:marklist}.
\begin{figure}
\begin{center}
<<>>=
<<marklist>>
@
\end{center}
\caption{Sample mark list produced by the \texttt{marklist()} function. \label{fig:marklist}}
\end{figure}

Similarly, the \verb!multiplechoice()! function produces a box
suitable for manually entering multiple choice answers.  This would be
used on a test that only had a few multiple choice questions (Figure \ref{fig:multiplechoice}).
\begin{figure}
\begin{center}
<<>>=
multiplechoice(1:10)
@
\end{center}
\caption{Multiple choice response box produced by \texttt{multiplechoice(1:10)}. \label{fig:multiplechoice}}
\end{figure}

\subsection{Producing an answer key}

A side effect of using the functions described in section \ref{sec:questions} is that information on the
randomization is saved
and is used to produce an answer key.  (This information is also used in the 
test analysis functions; see section \ref{sec:analysis}.) For example, the answer key from the questions
above is requested via

\noindent\verb!Answer key for version \Sexpr!\verb!{versioncodes[testversion]}:\\!
\begin{verbatim}
 <<>>=
 answerkey(symbols=LETTERS)
 @
\end{verbatim}
and displayed as \\

Answer key for version \Sexpr{versioncodes[testversion]}:\\
<<>>=
answerkey(symbols=LETTERS)
@

\section{Reading Student Responses}
\label{sec:reading}

\section{Analysis of a Test}
\label{sec:analysis}

The report uses the \pkg{tables} package \citep{pkg:tables}. This section will discuss the individual analysis tools used
during the report creation process. The actual report creation process is discussed in the next section.

This package utilizes a sample data set to illustrate the statistics that can be calculated.

<<echo=TRUE>>=
Sample <- system.file(''sample/sample.dat'', package = ``Sweavetest'')
@

Using the results of the \verb!readScanex()! and the \verb!grades()! functions, a report analyzing the test as a whole
and each question individually can be easily created. The functions are called by using:

\begin{verbatim}
<<<>>=
scanex <- readScanex(''Sample'')
GradedTests <- grades(scanex)
@
\end{verbatim}

There are several individual functions that are used during the report creation process. Each is discussed below.

\subsection{Statistical Overview}

First, the report process creates a statistical overview of the test using the \verb!StatisticalOverview()! function.
This function is called very simply, taking only the result of the \verb!grades()! function as input:

\begin{verbatim}
StatisticalOverview(GradedTests)
\end{verbatim}

This function is used primarily for formatting in \LaTeX, utilizing the \pkg{tables} package to format the created tables
and called functions. 

When called, this package calls several functions that create the statistics which analyze the overall quality of the test. The provided statistics are:

\begin{itemize}
\item Descriptive Statistics Table
\item Student Count Table
\item Test Means Table
\item Kuder-Richardson 20 Statistic
\item Ferguson's Delta
\item Answer Correlation Plot
\item Histogram of Student Scores
\end{itemize}

\subsection{Descriptive Statistics}

The descriptive statistics table is created within the \verb!StatisticalOverview()! function and includes each of the following statistics
based on the results of the test:

\begin{itemize}
\item Mean
\item Standard Deviation
\item Maximum Score
\item Minimum Score
\item 25th Quantile
\item Median
\item 75th Quantile
\end{itemize}

The table presents these statistics seperated by class section (if there is more than one section) and also presents them aggregated.
\LaTeX formatting is done using the \pkg{tables} package.

The table is created within the \verb!StatisticalOverview()! function using the following syntax:

\begin{verbatim}
Mean <- function(x) mean(x)
Quantile25 <- function(x) quantile(x, .25)
Quantile75 <- function(x) quantile(x, .75)
Median <- function(x) quantile(x, .50)
Max <- function(x) max(x)
Min <- function(x) min(x)
StdDev <- function(x) sqrt(var(x))

latex(tabular((Heading(Section)*factor(Section)+1) ~ Format(digits=2)*((Heading())*Percentage*Mean+(Heading())*Percentage*StdDev
              +(Heading())*Percentage*Max+(Heading())*Percentage*Quantile75+(Heading())*Percentage*Median
              +(Heading())*Percentage*Quantile25+(Heading())*Percentage*Min), data=GradedTests))
\end{verbatim}

Which produces the following table:

<<>>=

Mean <- function(x) mean(x)
Quantile25 <- function(x) quantile(x, .25)
Quantile75 <- function(x) quantile(x, .75)
Median <- function(x) quantile(x, .50)
Max <- function(x) max(x)
Min <- function(x) min(x)
StdDev <- function(x) sqrt(var(x))
Percentage <- grades/weight
weight <- max(seq_len(max(nchar(GradedTests$Correct))))

latex(tabular((Heading(Section)*factor(Section)+1) ~ Format(digits=2)*((Heading())*Percentage*Mean+(Heading())*Percentage*StdDev
              +(Heading())*Percentage*Max+(Heading())*Percentage*Quantile75+(Heading())*Percentage*Median
              +(Heading())*Percentage*Quantile25+(Heading())*Percentage*Min), data=GradedTests))

@

\subsection{Student Count And Test Means Tables}

The student count table provides a count of the number of students who wrote each different version of the exam, separated into class section
and aggregated, providing a total count for each exam code, and a grand total. This is produced automatically when the \verb!StatisticalOverview()!
function is called, with \LaTeX formatting using the \pkg{tables} package.

The test means table calculates the overall test mean for each unique exam code and class section, as well as aggregate totals and a grand total.
This tool is in place to help the instructor ensure that all test versions and class sections scored approximately the same. Any large discrepancies
should be investigated.

Using the \pkg{tables} it is quite easy to produce these two tables:

\begin{verbatim}
The Student Count table is produced by:

latex(tabular((Heading("Exam Code")*factor(ExamCode)+1) 
              ~ (Heading(Section)*factor(Section)+1), data=GradedTests))

And the Test Means table is produced by:

pct <- function(x) 100*mean(x)/weight

latex(tabular((Heading("Exam Code")*factor(ExamCode)+1) 
              ~ (Heading(Section)*factor(Section)+1)*Heading()*Heading()*Grade
              *pct*Format(digits=3), data=GradedTests))

\end{verbatim}

The resulting tables look like this:

<<>>=

  cat("\\begin{table}[h]")
  cat("\\caption{Student Counts and Test Means}")
  cat("\\begin{subtable}[h]{.5\\linewidth}")
  cat("\\centering")
  cat("\\caption{Student Counts}")
  latex(tabular((Heading("Exam Code")*factor(ExamCode)+1) 
              ~ (Heading(Section)*factor(Section)+1), data=GradedTests))
  cat("\\end{subtable}")
  cat("\\begin{subtable}[h]{.5\\linewidth}")
  cat("\\centering")
  cat("\\caption{Test Means}")
  latex(tabular((Heading("Exam Code")*factor(ExamCode)+1) 
              ~ (Heading(Section)*factor(Section)+1)*Heading()*Heading()*Grade
              *pct*Format(digits=3), data=GradedTests))
  cat("\\end{subtable}")
  cat("\\end{table}")

@

\subsection{Kuder-Richardson 20}

The Kuder-Richardson 20 statistics is produced by calling the \verb!KR20()! function, available in the \pkg{Sweavetest} package.
It uses the result of \verb!grades()! as input and can be called as follows:

\begin{verbatim}
<<>>=
KR20(GradedTests)
@
\end{verbatim}

The result of calling this function is a single test statistic:

<<>>=
KR20(GradedTests)
@

This is a well known statistic that measures the internal reliability of the test. It measures how consistently the items on the 
test provide information about the knowledge of the students on the tested material.

It is calculated through the following simple formula:

\[\alpha = \frac{K}{K+1}(1- \frac{\sum_{i=1}^{K} p_{i}q_{i}}{\sigma^{2}_{X}} \]

where K is the total number of items and \[p_{i}\] is the probability of success on item i.

\subsection{Ferguson's Delta}

The Ferguson's Delta statistic is produced by calling the \verb!FergusonsDelta()! function, available in the \pkg{Sweavetest} package.
It takes the results of \verb!grades! as input, and can be called by the user as follows:

\begin{verbatim}
<<>>=
FergusonsDelta(GradedTests)
@
\end{verbatim}

The result of this function is also a single test statistic:

<<>>=
FergusonsDelta(GradedTests)
@

Similar to the KR20 statistic, Ferguson's Delta also measures the consistency of the test. However, instead of measuring internal
reliability, it measures test reliability as ``the ratio between the number of discriminations made by the test to the greatest number
of discriminations that the test can generate, given the size of the sample and the number of items'' (Kline, 1986). 

It is calculated using the following easy formula:

\[ \delta = \frac{(m+1)(n^{2}-\sum(f^{2}_{s}}{mn^{2}}\]

where n is the number of test takers, m is the number of items, and \[f_{s}\] is the number of test takers obtaining each possible test score.

Both the Ferguson's Delta and Kuder-Richardson 20 statistics are formatted into a table using the \pkg{tables} package.

\subsection{Answer Correlation Plot}

% % % % %Dr. Murdoch to Fill In % % % % % % % %

\subsection{Score Distribution}

The \verb!StatisticalOverview()! function also produces a histogram of the scores achieved by the students on the test.
The purpose of this plot is to show the instructor the distribution of his test scores, and to help identify possible
problems with the test as a whole (when used in conjunction with the other statistics provided).

It is created as follows in the \verb!StatisticalOverview()! function:

\begin{verbatim}
weight <- max(seq_len(max(nchar(GradedTests$Correct))))
Percentage <- 100*GradedTests$Grade/weight
GradeHistogram <- hist(Percentage, main="Histogram of Student Scores", breaks = c(0,10,20,30,40,50,60,70,80,90,100))
\end{verbatim}

The resulting plot is shown below:

<<>>=
weight <- max(seq_len(max(nchar(GradedTests$Correct))))
Percentage <- 100*GradedTests$Grade/weight
GradeHistogram <- hist(Percentage, main="Histogram of Student Scores", breaks = c(0,10,20,30,40,50,60,70,80,90,100))
@

\subsection{Individual Question Analysis}

The \pkg{Sweavtest} package also analyzes each question individually, providing a number of useful statistics to aid in the 
recognition of potential problem questions. This includes questions that have been keyed incorrectly, questions that have
more than one plausible answer, questions that are misleading or ambiguous, and questions that may simply be too difficult.

Analysis is accomplished using the following statistics:

\begin{itemize}
\item Difficulty Rating
\item Item Discrimination
\item Point Biserial Correlation
\item Option Frequency
\item Distractor Discrimination
\item Option Frequency Plot
\item Empirical Probability Plot
\end{itemize}

Each of these items will be discussed in detail.

\subsection{Difficulty Rating}

Difficulty rating measures the number of respondents who answered the question correctly,
as a percentage of all respondents. The formula for difficulty rating is very simple:

\[ Difficulty Rating = \frac{C}{W + C} \]

where C is the number of correct respondents, and W is the number of wrong respondents.

Difficulty rating can be easily calculated using the built in function \verb!DifficultyRating()!.
It takes the result of the \verb!grades()! function discussed above, and can be called as follows:

\begin{verbatim}

DifficultyRating(GradedTests)

\end{verbatim} 

This function produces one difficulty rating per question:

<<>>=
DifficultyRating(GradedTests)
@

A question with a difficulty rating less than .2, or greater than .8 should be reviewed by the instructor.

\subsection{Item Discrimination}

Item discrimination is used to determine if the question adequately distinguishes between those
students who have the required knowledge to answer the question and those who don't. It is calculated
very simply using the following formula:

\[ Item Discrimination = \frac{(Upper 25\% Correct) - (Lower 25\% Correct)}{Total Number of Respondents} \]

Where the upper 25\% of the students are defined to be those students who scored in the top 25\% on the test and got the question correct.
The lower 25\% of students are defined similarly.

Item discrimination can be calculated using the \verb!ItemDiscrimination()! function. It takes as input
the results of the \verb!grades()! function and can be called using the following syntax:

\begin{verbatim}
ItemDiscrimination(GradedTests)
\end{verbatim}

Similar to the outpur produced by the \verb!DifficultyRating()! function, one item discrimination value is produced per question:

<<>>=
ItemDiscrimination(GradedTests)
@

A question with a negative item discrimination value should be reviewed by the instructor.

\subsection{Point Biserial Correlation}

Point biserial correlation is also a measure of test discrimination. It is easily calculated through
the following formula:

\[ Point Biserial = \frac{M_{p} - M_{q}}{\sigma\sqrt{pq}} \]

where:

\begin{itemize}
\item \[M_{p}\] is the mean score of students answering the question correctly
\item \[M_{q{}\] is the mean score of students answering the question incorrectly
\item \[\sigma\] is the standard deviation of the exam mean
\item p is the proportion of students answering the question correctly
\item q is the proportion of students answering the question incorrectly
\end{itemize}

Point biserial correlation can be easily calculated using the \verb!PointBiserial()! function
included in the \pkg{Sweavetest} package. It takes as input the results of the \verb!grades()! function,
and is called using the following syntax:

\begin{verbatim}
PointBiserial(GradedTests)
\end{verbatim}

Again, this function creates one value of the point biserial index per question:

<<>>=
PointBiserial(GradedTests)
@

Point biserial correlation should be used in conjunction with the calculated item discrimination
statistic to determine if a question adequately discriminates between those students who have the
required knowledge and those who don't.

Any question with a point biserial value below .2 should be reviewed by the instructor. 

\subsection{Option Frequency}

Using the results of the \verb!CreateIndex()! function, a count of how many students chose
each answer option is provided. This will aid the instructor in determining which answer choices were the
most popular amongst the students, and which were the least popular.

Any answer option which has very few respondents should be reviewed by the instructor.

\subsection{Distractor Discrimination}

An item discrimination value is also calculated for each of the answer options. This is accomplished
in the exact same way as described above (where it is done for the entire test). Ideally, the only 
answer option with a positive item discrimination value will be the correct answer. All other options
should have a negative item discrimination value, which tells the instructor that the students who scored
lowest on the test are choosing that option more frequently than the students who scored highest on the test.

Any incorrect answer with a positive item discrimination should be evaluated by the instructor.

Both the Option Frequency and Distractor Discrimination statistics are combined in a single table for convenience.

<<>>=

pct <- function(x) 100*mean(x)/weight

 cat("\\begin{table}[h]")
  cat("\\caption{Student Counts and Test Means}")
  cat("\\begin{subtable}[h]{.5\\linewidth}")
  cat("\\centering")
  cat("\\caption{Student Counts}")
  latex(tabular((Heading("Exam Code")*factor(ExamCode)+1) 
              ~ (Heading(Section)*factor(Section)+1), data=GradedTests))
  cat("\\end{subtable}")
  cat("\\begin{subtable}[h]{.5\\linewidth}")
  cat("\\centering")
  cat("\\caption{Test Means}")
  latex(tabular((Heading("Exam Code")*factor(ExamCode)+1) 
              ~ (Heading(Section)*factor(Section)+1)*Heading()*Heading()*Grade
              *pct*Format(digits=3), data=GradedTests))
  cat("\\end{subtable}")
  cat("\\end{table}")
@

\subsection{Option Frequency Plot}

The option frequency plot provides a way for the instructor to visualise which questions were most popular
amongst the test takers. Frequencies are provided as boxes, and are separated by exam code (if more than one
is given). The boxes of the correct answers are white, while the boxes of incorrect answers are shaded red.

The plot is created by calling the function \verb!answerPlots()!. This function takes as input a matrix of the
students' answers, a matrix of the corresponding correct answers, a vector of exam codes, and the question number
of the plot to be produced. For example, the following will produce an option frequency plot for the first question
of our sample data:

\begin{verbatim}
StudentAnswers <- GradedTests$Answers
CorrectAnswers <- GradedTests$Correct
ExamVersion <- GradedTests$ExamCode
QuestionCounter <- 1

answerPlots(StudentAnswers, CorrectAnswers, ExamVersion, QuestionCounter)
\end{verbatim}

The resulting plot for the first question is shown below:

<<>>=
StudentAnswers <- GradedTests$Answers
CorrectAnswers <- GradedTests$Correct
ExamVersion <- GradedTests$ExamCode
QuestionCounter <- 1

answerPlots(StudentAnswers, CorrectAnswers, ExamVersion, QuestionCounter)
@

\subsection{Empirical Probability Plot}

The empirical probability plot represents the cumulative distribution function for the probability of
answering the question correctly. It plots the test scores along the x-axis and the probability that
the test taker will answer the question correctly along the y-axis.

From this graph, we can deduce the empirical probability of a student answering the question correctly
given his score on the test.

It would be expected that this graph would be monotonically increasing, from the lowest score achieved on
the test to the highest score.

The plot can be created using the \verb!EmpiricalProbabilityPlot()! function. It takes as input the results
from the \verb!grades()! function, as well as the question number for which the plot will created. A plot
for the first question can be created using the following syntax:

\begin{verbatim}
QuestionCounter <- 1
EmpiricalProbabilityPlot(GradedTests, QuestionCounter)
\end{verbatim}

Yielding the following graph:

<<>>=
QuestionCounter <- 1
EmpiricalProbabilityPlot(GradedTests, QuestionCounter)
@

A question in which the plot exhibits a decreasing trend overall as opposed to an increasing trend should
be reviewed by the instructor.

\subsection{Warnings}

Part of the analysis of the multiple choice test is providing the instructor with information on the
appropriateness of the calculated values of the statistics. If the values of the Item Discrimination, 
Difficulty Rating, or Point Biserial statistics fall below industry standards, 
the built in function \verb!Warnings{}! will automatically produce a message informing the instructor,
as well as pointing the instructor to resources with suggestions on improving (or possibly eliminating) 
said question.

Warnings are produced if any combination of the following occur:

\begin{itemize}
\item The item discrimination statistic falls below 0
\item The difficulty rating falls below 0.2
\item The point biserial correlation falls below 0.2
\end{itemize}

The \verb!Warnings()! function takes the calculated values of the item discrimination, difficulty
rating, and point biserial correlation as inputs. The syntax is as follows:

\begin{verbatim}
DR <- DifficultyRating(GradedTests)
ID <- ItemDiscrimination(GradedTests)
PB <- PointBiserial(GradedTests)

Warnings(DR, ID, PB)
\end{verbatim}

A number of different outputs can be produced dependent upon which statistics are at unacceptable
levels. A couple examples are given below:

If a single statistic is below acceptable standards:

<<>>=
DR <- 0
Warnings(DR, ID, PB)
@

If there are two statistics that are below acceptable standards:

<<>>=
DR <- 0
PB <- 0

Warnings(DR, ID, PB)
@

If all three statistics are below acceptable standards:

<<>>=
DR <- 0
PB <- 0
ID <- -1

Warnings(DR, ID, PB)
@

The report also provides warning messages if the Kuder-Richardson 20 statistic or Ferguson's Delta
fall below acceptable standards. These messages are produced directly by the \verb!StatisticalOverview()!
function.

Warnings are produced if the KR20 statistic falls below 0.7 or Ferguson's Delta falls below 0.9 and are 
given on the Statistical Overview page.

\section{Report Creation}
\label{sec:report}

The report creation process was designed to be very simple. There are two separate methods for the instructor
to choose from - the full report, and the analysis only report. Both are discussed below.

\subsection{Full Report}

The full report is to be used by those instructors who have created their entire test using the provided 
Sweave template. The creation of the test itself is discussed in the \ref{sec:preparing} section. 

By simply modifying a few global variables in the template, the instructor can create a full report
to analyze the results of the test.

The template provides a number of variables that are under the users control:

\begin{verbatim}
<<>>=
set.seed(18)
setwd(''C:\Users\Sample")
library(Sweavetest)
library(tables)
testversion <- 4
Version <- version("Student")
NumberOfVersions <- 4

if(Version == ``Report''){
scanex <- readScanex(''sample.dat'')
}
@
\end{verbatim}

To create the report, the user needs to change the following:
\begin{itemize}
\item The Version variable to ``Report''
\item The default ``sample.dat'' input for \verb!readScanex()! function to the Scantron file they wish to use.
\item The working directory must be changed to that containing the Scantron file
\end{itemize}

The variables not mentioned above should not be changed or modified in any way.

From here, everything else is achieved automatically through the built in functions.
The instructor need only process the newly modified .Rnw file, and a pdf report will be created.
The report will include reproduction of the original questions, with answers given in un-randomized
order, as well as all of the analysis described in the \ref{sec:analysis} section.

An appendix is also provided at the end of the report. The appendix contains brief explanations of
all the statistics used in the report. This will allow the instructor to quickly and efficiently
look up the meaning of any of the statistics, as well as suggestions on how to improve
the question if the calculated statistics were found to be below acceptable standards.

References are also provided if the instructor wants more information on any of the statistics
in the report.

\subsection{Analysis Only}

If the instructor did not use the Sweave template to create their test, analysis is still possible
using the \pkg{Sweavetest} package. 

If the user only has the results of the test, they should use the low level template provided with
the package. This template produces many of the same statistics as the full report, but it will not 
be able to reproduce the questions and answer options, nor will it be able to undo any randomization
in the answer options.

This method does however, provide a function that will allow the report to recognize the correspondence
between randomization in questions. The function that must be called is \verb!Unscramble()! function.

The \verb!Unscramble()! function takes the following as input:

\begin{itemize}
\item The result of the \verb!readScanex()! function
\item A vector of exam codes
\item A matrix of question orders, with rows corresponding to the order the exam codes were entered
\item A variable called Scramble, which takes on FALSE if there is no randomization in the question order, and TRUE otherwise
\end{itemize}

The syntax for using this function is as follows:

\begin{verbatim}
scanex <- readScanex(''sample.dat'')
ExamCodes <- c(170,738,840,967)
Master <- c(1:30)
Order1 <- c(15:30, 1:14)
Order2 <- c(20:30,1:10, 15:19,11:14)
Order3 <- c(30:1)
Orders <- matrix(c(Master,Order1,Order2,Order3), nrow=4, ncol=30, byrow=TRUE)

NewScanex <- Unscramble(scanex,ExamCodes,Orders,Scrambled = TRUE)

\end{verbatim}

we can see below what the function does by comparing the original Scantron file to the new scantron file

<<>>=
scanex <- readScanex(''Sample'')
ExamCodes <- c(170,738,840,967)
Master <- c(1:30)
Order1 <- c(15:30, 1:14)
Order2 <- c(20:30,1:10, 15:19,11:14)
Order3 <- c(30:1)
Orders <- matrix(c(Master,Order1,Order2,Order3), nrow=4, ncol=30, byrow=TRUE)

NewScanex <- Unscramble(scanex,ExamCodes,Orders,Scrambled = TRUE)
@

<<echo=TRUE>>=

#First 5 lines of old scanex
head(scanex)

#First 5 lines of new scanex
head(NewScanex)
@

We can clearly see that the answers have been re-ordered in the new Scantron file as specified by the
instructor. This will allow the \verb!LowLevelReport()! function to create a report with the proper
correspondences.

Note: This function is unnecessary when running a full report, as question randomization is not done.

The provided Sweave template provides several variables that are under the users control:

\begin{verbatim}
options(width=60)
dir.create("figs", showWarn=FALSE)
library(Sweavetest)
library(tables)
setwd("C:/Users/Sample")
scanex <- readScanex("sample.dat")
Unscramble(
LowLevelReport(scanex)
Appendix(Report == TRUE)
\end{verbatim}

Most of these variables can be left untouched. The user must only make two modifications
to the variables above:

\begin{itemize}
\item The working directory must be changed to the location of the Scantron file that will be read in
\item The file in the \verb!readScanex()! function must be changed to the desired Scantron file
\end{itemize}

No other variables should be modified, unless the instructor is familiar with \LaTeX formatting and 
wishes to change the pre-set formatting done by the template.

At this point the instructor need only process the Sweave document, and a report will be created
in the working directory.

\subsection{Report Without Using A Scantron}

If the instructor uses grading technology other than that provided by Scantron, they may still use
the analysis in the \pkg{Sweavetest} package, but must modify their input file to meet the specifications of 
the functions.

The instructor can create a .dat file with the following columns:

\begin{itemize}
\item Column 1 must contain the Student IDs
\item Column 2 must contain the Section of the each student
\item Column 3 must contain the Exam Codes for the test
\item Column 4 must be blank. When read in from the Scantron file, this column is designated as ``Marker'', but is unused by the \pkg{Sweavetest} package
\item Column 5 must count the number of exams that were taken
\item Column 6 must be the students' answers concatenated into a single character
\end{itemize}

An example file is given below. For best results, the users file should mirror the contents of this file as closely as possible, including
column titles.

<<>>=
head(scanex)
@

Note in the file above that the first four entries are the answer keys for the exam. These must be present in the Scantron file and are recognized by
the package by having Student ID 999999999. The answer keys do not require a section number, and should not count in the total exam count.

Once this file has been created, the instructor can carry on with the report process as described above (with either the full report or using the
\verb!LowLevelReport()! function).

\section{Cheating Detection}
\label{sec:cheating}

\begin{figure}
\begin{center}
<<fig=TRUE>>=
x <- 0:7
y <- 1:8
df <- expand.grid(x=x,y=y)
df$version <- with(df, 2*(y %% 2) + (x %% 2) + 1)
par(mar=c(0,0,0,0))
plot(y ~ x, pch = as.character(version), data=df, axes=FALSE,
      xlab=NULL, ylab=NULL, cex=1.2)
@
\caption{Seating pattern for four test versions to avoid having a student sit next
to another student with the same version. \label{fig:seating}}
\end{center}
\end{figure}

A common form of cheating on multiple choice tests is simply to copy the answers from a 
neighbour's or confederate's paper.  \pkg{Sweavetest} produces multiple versions of the tests
in order to make copying more difficult; with four test versions distributed in the pattern shown
in Figure \ref{fig:seating}, no student sits next to another student with the same version of the test.
The randomization functions described in section \ref{sec:questions} are designed so that
randomized answers in up to four different versions of the test end up different;  thus 
a student who copies from another student who has the correct answer will likely get the question
wrong.

To detect this kind of cheating, the \verb!wrongKey()! function re-marks each test using
all the other answer keys.  If a student has a higher score when marked with a different
answer key, it's a sign of either copying from a neighbour, or miscoding the answer key.


\section{Future Directions}

\pkg{Sweavetest} was originally developed by the first author (Murdoch) to support
his own teaching needs; only in 2012 was an attempt made to make it more generally useful,
so that others could use it.  There are still several changes planned to support this
direction.  
\begin{itemize}
\item The randomization of multiple choice items is designed for running four versions of the
tests.  Each version will have a different correct answer (unless the answer is one
of the unrandomized ones, such as ``none of the above'').  However, this scheme allows
a clever cheater to learn something by seeing what his/her neighbours have written, and
some instructors would prefer independent randomizations.

\item Not all instructors are comfortable with Sweave and \LaTeX.  Support for \pkg{odfWeave}
\citep{pkg:odfWeave}
or other schemes for test preparation will be considered.

\item We do not currently assign seating for students.  While we do record seating arrangements,
the records are on paper and are not normally digitized.   If these records were digitized, then
the cheating detection could look for correspondences between neighbouring students,
rather than between a student and the answer key for a different version of the test.
Even without those records, it could look for unusual correspondents between all pairs of 
students, albeit with reduced detection power.

\end{itemize}

\section*{Acknowledgments}

This research was supported by an NSERC Discovery Grant to the first author, and an
NSERC Undergraduate Summer Research Award to the second author.

\section{Not covered yet}

Update this list as we cover things.

Still need to document

 [1] "answerCorrelations"          "answerMatrix"        "clean"             
 [6]          "getglobal"          "grades"                              
[11]       "mergeLists"                         
[16] "nominalRoll"        "perms"                "readScanex"           
[26] "versioncodes"   


\bibliography{Sweavetest}

\end{document}

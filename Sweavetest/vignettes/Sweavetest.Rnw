\documentclass[12pt]{article}
\usepackage{Sweave}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{verbatim}

\usepackage[authoryear,round,comma]{natbib}
\bibliographystyle{apa}

\setlength{\oddsidemargin} {0.0in} 
\setlength{\textwidth} {6.5in}

%\VignetteIndexEntry{About the Sweavetest package}

% The next line is needed for inverse search...
\SweaveOpts{echo=TRUE,results=verbatim} % to show R code and results
\SweaveOpts{concordance=TRUE, keep.source=TRUE,echo=FALSE,results=tex}
<<>>=
options(width=70)
set.seed(18)
Sys.setenv(R_KEEP_PKG_SOURCE="yes")
install.packages("..",type="source",repos=NULL)
library(Sweavetest)
library(tables)
randomize(TRUE)
testversion(1)
newCommands(thesection=FALSE)
CheckDups(TRUE)
@

\newcommand{\pkg}[1]{{\fontseries{b}\selectfont #1}}

% These commands affect the display of the test samples
\newcommand{\Correct}{*}
\renewcommand{\labelenumii}{(\Alph{enumii})}

\title{The \pkg{Sweavetest} Package\footnote{This 
vignette  was built using \pkg{Sweavetest} version \Sexpr{packageDescription("Sweavetest")$Version}}.}%$

\author{Duncan Murdoch \and Adam Rahman}

\begin{document}

\maketitle

\abstract{

Multiple choice tests are used frequently to assess student
performance in large classes. The \pkg{Sweavetest} package
contains support functions to format, grade and analyze multiple
choice tests.  Answers are randomized during formatting,
and after grading a report is automatically generated, showing descriptive
statistics, measuring test reliability, and presenting item analysis.
Errors in answer keys and poorly worded questions are easily
recognized through the information generated. Several plots are also
created to visually show the instructor how well the students are
performing, to help identify possible problem questions,
and to detect answer patterns that suggest cheating.
}

\tableofcontents

\section{Introduction}

The \pkg{Sweavetest} package is designed to support administration of multiple choice
tests.  It was written at the University of Western Ontario (UWO), and some aspects of the code
reflect the way we administer tests, but efforts have been made to make it more generally
useful.

There are four main parts to the package, described in the following sections.  First, a
number of functions are given to help an instructor prepare a test using Sweave \citep{Leisch2002}.
These are described in section \ref{sec:preparing}.  At UWO, the students use Scantron 
mark-sense sheets to record answers on multiple-choice tests; section \ref{sec:reading}
describes how these are handled in \pkg{Sweavetest}, producing grades for each student
and merging them into existing class lists.

An important part of testing is to analyze the results of a test.  This can detect badly
worded questions, errors in answer keys, and can otherwise give information about the
quality of the questions and of the test as a whole.  Section \ref{sec:analysis} describes
this aspect of \pkg{Sweavetest}.  It may also be possible to use the aggregate results
to detect cheating by individual students; section \ref{sec:cheating} describes how this is done.
This paper finishes with a short discussion of plans for changes to \pkg{Sweavetest}.


\section{Preparing a Test}
\label{sec:preparing}

\pkg{Sweavetest} was originally designed to support writing multiple
choice tests in Sweave and \LaTeX, and that is still its main purpose.
There are functions that output introductory material at the start of
the test, as well as functions that format and randomize the answers.
These functions also record the results of the randomization so that
an answer key can be produced automatically at the end.

Using the test preparation functions requires knowledge of Sweave and \LaTeX.  Instructors
who want to use other document preparation systems to produce the test can skip this section,
but some of the work done here automatically will have to be done manually later.

A complete test prepared using \pkg{Sweavetest} is included in the 
\texttt{sample} directory in the \texttt{SampleTest.Rnw} file.

\subsection{Writing questions}
\label{sec:questions}

The functions to randomize answers all work on the responses to questions; the question text
itself is not important, but it is assumed that all questions have multiple choice responses.  See below for how to handle tests where some questions are in other formats.

\pkg{Sweavetest} supports four separate versions of each test; questions
may contain from 2 to 5 answers.  The preamble to the test randomizes the test version
numbers to be 4  numbers from 100 to 999 in sorted order.  Version 1 of the test has the smallest 
test version number, version 4 the largest.

The supplied functions do the work of formatting the answers.   For example,
\begin{verbatim}
 \item Let $U \sim \Unif(0,1)$.  Set $X$ equal to 0 if $U > 0.3$,
       equal to 1 otherwise.  The distribution of $X$ is
 <<>>=
 enumerate("Binomial$(n=1, p=0.7)$", 
      "Binomial$(n=1, p=0.3)$",
      "Poisson$(\\mu = 0.3)$",
      "Poisson$(\\mu = 0.7)$",
      "Exponential$(\\lambda=0.3)$",
      Correct=2)
 @
\end{verbatim}
The \verb!enumerate()! function produces
an enumeration environment in \LaTeX.  The answers are displayed in a random order.  Earlier in the text, we've used
\begin{verbatim}
 \renewcommand{\labelenumii}{(\Alph{enumii})}
\end{verbatim}
so that the the answers are labelled with 
capital letters.  (We have also initialized the test to mark the correct answer
with an asterisk. We wouldn't do this on the version of the test we give to 
students!  See section \ref{sec:initialization} for details.) 
We also used \verb!\SweaveOpts{echo=FALSE,results=tex}! earlier to set the defaults
to output \LaTeX\ code without echoing the input.   With this preparation, the Sweave code listed
above will be processed and displayed as
\begin{enumerate}
\item Let $U \sim \Unif(0,1)$.  Set $X$ equal to 0 if $U > 0.3$, equal to 1 otherwise.  The
distribution of $X$ is
<<>>=
Version("Teacher")
QuestionCounter(0)
enumerate("$\\Binom(n=1, p=0.7)$", 
      "$\\Binom(n=1, p=0.3)$",
      "$\\Poisson(\\mu = 0.3)$",
      "$\\Poisson(\\mu = 0.7)$",
      "$\\Exp(\\lambda=0.3)$",
      Correct=2)
@
\end{enumerate}

Besides \verb!enumerate()!, several other functions produce randomized answer lists.  The
\verb!horiz()! function formats them horizontally on a single line.  The \verb!items()! function
is like \verb!enumerate()!, but only the \LaTeX\ \verb!\items! part of the enumeration is output:
it is assumed that the test writer has already entered \verb!\begin{enumerate}! or equivalent.
The \verb!things()! function just randomizes the items it is given; the test writer needs to
enter the \verb!\item! values if necessary.

The \pkg{Sweavetest} package also has special support for questions
involving R code. \verb!Renumerate()!, \verb!Rhoriz()! and 
\verb!Ritems()! format text to look like R input expressions. The 
\verb!resultenumerate()! and \verb!resultitems()! function evaluate the
arguments and format the output as if it was printed as output by R,
unless they are protected by being wrapped in the \verb!msg()! function.
For example, the input
\begin{verbatim}
 \item Which of the following gives the best approximation to 
 $\int_0^5 (x + 4) \cos(x) dx$?
 <<>>=
 Renumerate(Correct=2,
 "x <- runif(1000, 0, 5); mean((x + 4)*cos(x))",
 "x <- runif(1000, 0, 5); mean(5*(x + 4)*cos(x))",
 "x <- runif(1000, 0, 5); mean((x + 4)*cos(x)/5)",
 "x <- rcos(1000, 0, 5); mean(x + 4)",
 "42")
 @
\end{verbatim}
produces
\begin{enumerate}
\setcounter{enumi}{1}
\item Which of the following gives the best approximation to $\int_0^5 (x + 4) \cos(x) dx$?
<<>>=
 Renumerate(Correct=2,
 "x <- runif(1000, 0, 5); mean((x + 4)*cos(x))",
 "x <- runif(1000, 0, 5); mean(5*(x + 4)*cos(x))",
 "x <- runif(1000, 0, 5); mean((x + 4)*cos(x)/5)",
 "x <- rcos(1000, 0, 5); mean(x + 4)",
 "42")
@ 
\end{enumerate}
and 
\begin{verbatim}
 \item What will the following code produce?
 <<results=hide,echo=TRUE>>=
 x <- rep(c(TRUE, FALSE), 2)
 y <- rep(c(TRUE, FALSE), each=2)
 x & !y
 @
 <<>>=
 resultenumerate(Correct=1, KeepLast=1, x & !y, x & y,
 x | !y, x | y, msg("None of the above"))
 @
\end{verbatim}

\begin{enumerate}
\setcounter{enumi}{2}
\item What will the following code produce?
<<results=hide,echo=TRUE>>=
x <- rep(c(TRUE, FALSE), 2)
y <- rep(c(TRUE, FALSE), each=2)
x & !y
@
<<>>=
resultenumerate(Correct=1, KeepLast=1,
x & !y,
x & y,
x | !y,
x | y,
msg("None of the above"))
@
\end{enumerate}

If some question numbers are not used for multiple choice questions, then the test author can
manipulate the internal question counter using the \texttt{QuestionCounter()} global.  See
section \ref{sec:globals}.

\subsection{Introductory material at start of test}
\label{sec:initialization}

Normally the first R code to appear in a test is
\begin{verbatim}
 <<>>=
 set.seed(SEED)
 library(Sweavetest) 
 @
\end{verbatim}
where \texttt{SEED} is replaced by a fixed number.  This initializes the package
and fixes the randomization
of the test questions so that Sweavetest will produce the same results on future runs.

Next, one calls the \texttt{Initialization()} function, for example
\begin{verbatim}
 <<>>=
 Initialization(name="Test1", version="Teacher")
 @
\end{verbatim}
This names the test
so that when Sweavetest saves files associated with the test, it will use that name, and 
sets the \texttt{Version()} global variable.  Normally the \texttt{Version()} is set
to ``Teacher'' during preparation of the test; this will display correct answers,
and an answer key at the end.  Use ``Student'' to produce a randomized version for the
students, and ``Report'' after the test is complete to produce a report and
analyze responses.  (More details on ``Report'' are given in section \ref{sec:report}.)
It also sets up the 
internal Sweavetest variables.  

\begin{table}
\caption{Current math mode definitions provided by \texttt{newCommands()}.\label{tab:commands}}
\begin{center}
\begin{tabular}{lc}
\hline
Macro & Sample \\
\hline
<<>>=
n <- names(formals(newCommands))
for(i in n) {
  if (!(i %in% c("Marks", "thesection", "lowtilde")))
    cat("\\verb!\\", i, "! & $ \\", i, " $ \\\\ \n", sep="")
}
@
\hline
\end{tabular}
\end{center}
\end{table}
The \verb!Initialization()! call also calls
the \verb!newCommands()! function to define a number of \LaTeX\ macros that may be
convenient to use in the test.  Current definitions of simple macros
to use in mathematical expressions are shown in Table \ref{tab:commands}.  The function
also optionally defines \verb!\Marks!, which displays marks for a question in the right
margin, \verb!\thesection!, which suppresses section numbering, and \verb!\lowtilde!,
which draws a tilde under a math symbol, similar to the convention used on a blackboard
to indicate a vector.  By default, all macros are defined, but any of them
may be suppressed by passing \verb!FALSE! to the corresponding argument to 
\verb!newCommands()!.  For example,
this document uses \verb!newCommands(thesection = FALSE)!, because section headings
should not be overridden.

Some other functions are provided for the convenience of test production.
The \verb!marklist()! function outputs a box, suitable for recording marks on the
first page of a test.  For example,
<<marklist, echo=TRUE, eval=FALSE>>=
marklist(marks=rep(5,4), names=1:4)
@
produces the text and box shown in Figure \ref{fig:marklist}.
\begin{figure}
\begin{center}
<<>>=
<<marklist>>
@
\end{center}
\caption{Sample mark list produced by the \texttt{marklist()} function. \label{fig:marklist}}
\end{figure}

Similarly, the \verb!multiplechoice()! function produces a box
suitable for manually entering multiple choice answers.  This would be
used on a test that only had a few multiple choice questions (Figure \ref{fig:multiplechoice}).
\begin{figure}
\begin{center}
<<>>=
multiplechoice(1:10)
@
\end{center}
\caption{Multiple choice response box produced by \texttt{multiplechoice(1:10)}. \label{fig:multiplechoice}}
\end{figure}

\subsection{Producing an answer key}

A side effect of using the functions described in section \ref{sec:questions} is that information on the
randomization is saved
and is used to produce an answer key.  (This information is also used in the 
test analysis functions; see section \ref{sec:analysis}.) For example, the answer key from the questions
above is requested via

\noindent\verb!Answer key for version \Sexpr!\verb!{versioncode()}:\\!
\begin{verbatim}
 <<>>=
 answerkey()
 @
\end{verbatim}
and displayed as \\

Answer key for version \Sexpr{versioncode()}:\\
<<>>=
answerkey(symbols=LETTERS)
@

\section{Reading Student Responses}
\label{sec:reading}

At UWO, multiple choice tests are normally recorded on ``Scantron'' sheets.  Scantron is a
commercial company producing products for data entry; the sheets used at UWO are customized
to our needs (Figure \ref{fig:scantron}).  Each sheet records a student number, section, exam
code, and up to 180 multiple choice answers (60 on the front, 120 on the back) using mark-sense
technology.  The student's name and signature, and the instructor's name and the course number
are also written on the sheet.
\begin{figure}
\begin{center}
\includegraphics[width=5in]{scantron_front.pdf}
\end{center}
\caption{The front of a UWO Scantron sheet.\label{fig:scantron}}
\end{figure}

A scanner reads and interprets the mark-sense fields, producing a raw file of data, with one line
per sheet scanned.  The sheets are stamped with a sequence number that is also recorded in the file.
The format of the file is shown in Table \ref{tab:scantron}.  Columns not listed contain
punctuation that we ignore.

\begin{table}
\caption{Fixed format of Scantron raw data file.\label{tab:scantron}}
\begin{center}
\begin{tabular}{clclc}
\hline
Columns & \multicolumn{1}{c}{Contents} & \multicolumn{3}{c}{\texttt{readScanex} column} \\
\hline
1--9 & Student id number & & \texttt{"Student ID"} & \\
11--13 & Course section number & & \texttt{"Section"} & \\
15--17 & Exam code number & & \texttt{"ExamCode"} & \\
18--19 & Answer sheet number & & \texttt{"Sheet"} & \\
21--24 & Sequence number of Scantron sheet & & \texttt{"Scantron"} & \\
26--205 & Recorded responses (or blanks) & & \texttt{"Answers"} & \\
\hline
\end{tabular}
\end{center}
\end{table}

The \texttt{readScanex()} function reads files in this format, producing a data frame with column names
as shown in the table.  All fields are read as character data.  None of the 
functions in \pkg{Sweavetest} make use of the \texttt{"Sheet"} field, as our tests always have fewer than
180 questions.

\section{Analysis of a Test}
\label{sec:analysis}

This section will discuss the individual analysis tools used
during the report creation process. The actual report creation process
is discussed in section \ref{sec:report}.

This package utilizes a sample data set to illustrate the statistics that can be calculated:

<<echo=TRUE>>=
Sample <- system.file("sample/sample.dat", package = "Sweavetest")
TestIndex.csv <- system.file("sample/TestIndex.csv", package = "Sweavetest")
@

Using the results of the \verb!readScanex()! and the \verb!grades()! functions, a report analyzing the test as a whole
and each question individually can be created. The functions are called using the following syntax:

\begin{verbatim}

scanex <- readScanex(Sample)
GradedTests(grades(scanex))

\end{verbatim}

<<>>=
scanex <- readScanex(Sample)
GradedTests(grades(scanex))
@

There are several individual functions that are used during the report creation process. Each is discussed in detail below.

\subsection{Statistical Overview}

First, the report process creates a statistical overview of the test using the \verb!StatisticalOverview()! function.
This function is called by taking the result of the \verb!grades()! function as input, and utilyzing the following syntax:

\begin{verbatim}
StatisticalOverview()
\end{verbatim}

This function is used primarily for formatting in \LaTeX, utilizing the \pkg{tables} package \citep{pkg:tables} to format the created tables
and called functions. 

When called, \verb!StaticalOverview()! calls several functions that
create the statistics which analyze the overall quality of the test. It produces the following 
statistics, described in the following sections.

\begin{itemize}
\item Descriptive Statistics Table
\item Student Count Table
\item Test Means Table
\item Kuder-Richardson 20 Statistic
\item Ferguson's Delta
\item Answer Correlation Plot
\item Histogram of Student Scores
\end{itemize}

\subsection{Descriptive Statistics}

The descriptive statistics table is created within the \verb!StatisticalOverview()! function and includes each of the following statistics
based on the results of the test:

\begin{itemize}
\item Mean
\item Standard Deviation
\item Maximum Score
\item Minimum Score
\item $25^{th}$ Quantile
\item Median
\item $75^{th}$ Quantile
\end{itemize}

The table presents these statistics separately by class section and also for the class as a whole.
This produces the output shown in Table \ref{tab:StatisticalOverview}.
\begin{table}
\caption{Summary statistics from the \texttt{StatisticalOverview} function. \label{tab:StatisticalOverview}}
\begin{center}
<<>>=
numQ <- max(nchar(GradedTests()$Correct))
grades <- GradedTests()$Grade
DescriptiveStatistics(100*GradedTests()$Grade/numQ, GradedTests()$Section)
@
\end{center}
\end{table}

\subsection{Student Count And Test Means Tables}

The student count table provides a count of the number of students who wrote each different version of the exam, separated into class section.
Aggregated totals including a total count for each exam code, and a grand total are also provided. This is produced automatically when the \verb!StatisticalOverview()!
function is called.

The test means table calculates the overall test mean for each unique exam code and class section, as well as aggregate totals and a grand total.
This tool is in place to help the instructor ensure that all test versions and class sections scored approximately the same. Any large discrepancies
should be investigated.

The resulting tables are displayed as Tables \ref{tab:StudentCount} and \ref{tab:TestMeans}
respectively.
\begin{table}
\caption{Student count table. \label{tab:StudentCount}}
\begin{center}
<<>>=
latex(tabular((Heading("Exam Code")*factor(ExamCode)+1) 
              ~ (Heading(Section)*factor(Section)+1), data=GradedTests()))
@
\end{center}
\end{table}
\begin{table}
\caption{Test means table. \label{tab:TestMeans}}
\begin{center}
<<>>=
pct <- function(x) 100*mean(x)/numQ

latex(tabular((Heading("Exam Code")*factor(ExamCode)+1) 
              ~ (Heading(Section)*factor(Section)+1)*Heading()*Heading()*Grade
              *pct*Format(digits=3), data=GradedTests()))
@
\end{center}
\end{table}

\subsection{Kuder-Richardson 20}

The Kuder-Richardson 20 statistic is produced by calling the \verb!KR20()! function.
It uses the result of \verb!grades()! as input and can be called as follows:

<<echo=TRUE, results=verbatim>>=
KR20(GradedTests())
@

The Kuder-Richardson 20 measures the internal reliability of the test. It measures how consistently the items on the 
test provide information about the knowledge of the students on the tested material.

It is calculated through the following formula:

\[ \alpha = \frac{K}{K+1}\left(1- \frac{\sum_{i=1}^{K} p_{i}q_{i}}{\sigma^{2}_{X}}\right) \]

\noindent where K is the total number of items, $p_{i}$ is the probability of success on item i, $q_{i}$ is the probability of failure on item i, and $\sigma^{2}$ is the variance of the student scores.

\subsection{Ferguson's Delta}

Ferguson's Delta statistic is produced by calling the \verb!FergusonsDelta()! function.
It takes the results of \verb!grades()! as input, and can be called by the instructor as follows:

<<echo=true, results=verbatim>>=
FergusonsDelta(GradedTests())
@

Similar to the KR20 statistic, Ferguson's Delta also measures the consistency of the test. However, instead of measuring internal
reliability, it measures test reliability as ``the ratio between the number of discriminations made by the test to the greatest number
of discriminations that the test can generate, given the size of the sample and the number of items" (Kline, 1986). 

It is calculated using the following formula:

\[ \delta = \frac{(m+1)(n^{2}-\sum(f^{2}_{s}))}{mn^{2}}\]

\noindent where n is the number of test takers, m is the number of items on the test, and $f_{s}$ is the number of test takers obtaining each possible test score.

\subsection{Answer Correlation Plot}

The answer correlation plot can be used by the instructor to quickly determine if a question is miskeyed or worded poorly. It plots difficulty rating along the x-axis, and the correlation between the students' total score and their answer on each question along the y-axis. The plotting characters are numerical and correspond to the question number so the instructor can quickly identify where each question is located on the plot. 

A good question that is also difficult should be located in the upper left quadrant of the plot. This question is very difficult, but those students answering correctly scored highest on the test. A good question of average difficulty is generally located in the center of the plot. A mix of all students are answering this question correctly, but mostly students scoring well on the test. A particularly easy question is located in the bottom right quadrant of the plot, as almost everyone is answering this question correctly, and therefore there is virtually no correlation between total score and question answer.

As an overall trend, the questions should form a line from the upper left corner to the bottom right corner.

The function \verb!answerCorrelations()! is used to create the answer correlation plot. It takes as input the answers provided by the students and the corresponding correct answers, and is called using the following syntax:

<<answerCorrelations, eval=false, echo=true>>=
answerCorrelations()
@
The resulting plot is shown in Figure \ref{fig:answerCorrelations}.
\begin{figure}
\begin{center}
<<fig=true>>=
<<answerCorrelations>>
@
\end{center}
\caption{Plot produced by the \texttt{answerCorrelations} function.\label{fig:answerCorrelations}}
\end{figure}

\subsection{Score Distribution}

The \verb!StatisticalOverview()! function also produces a histogram of the scores achieved by the students on the test.
The purpose of this plot is to show the instructor the distribution of the test scores, and to help identify possible
problems with the test as a whole (when used in conjunction with the other statistics provided).

The resulting plot is shown in Figure \ref{fig:ScoreDistribution}.
\begin{figure}
\begin{center}
<<fig=true>>=
Percentage <- 100*GradedTests()$Grade/numQ
GradeHistogram <- hist(Percentage, main="Histogram of Student Scores", 
                        breaks = c(0,10,20,30,40,50,60,70,80,90,100))

@
%$
\end{center}
\caption{The score distribution produced by the \texttt{StatisticalOverview} function.
\label{fig:ScoreDistribution}}
\end{figure}

\section{Individual Question Analysis}

The \pkg{Sweavetest} package also analyzes each question individually, providing a number of useful statistics to aid in the 
recognition of potential problem questions. This includes questions that have been keyed incorrectly, questions that have
more than one plausible answer, questions that are misleading or ambiguous, and questions that may simply be too difficult.

Analysis is accomplished using the following:

\begin{itemize}
\item Difficulty Rating
\item Item Discrimination
\item Point Biserial Correlation
\item Option Frequency
\item Distractor Discrimination
\item Option Frequency Plot
\item Empirical Probability Plot
\end{itemize}

Each of these items will be discussed in detail.

\subsection{Difficulty Rating}

Difficulty rating measures the number of respondents who answered the question correctly,
as a percentage of all respondents. The formula for difficulty rating is given as:

\[ \textit{Difficulty Rating} = \frac{C}{W + C} \]

\noindent where C is the number of respondents answering correctly, and W is the number of respondents answering incorrectly.

Difficulty rating can be calculated using the function \verb!DifficultyRating()!.
It takes the result of the \verb!grades()! function discussed above, and produces one difficulty rating per question.
It can be called as follows:

<<echo=true, results=verbatim>>=
DifficultyRating(GradedTests())
@

A question with a difficulty rating less than 0.2 has such a low success rate 
that it should be reviewed by the instructor.

\subsection{Item Discrimination}
\label{sec:ItemDiscrimination}

Item discrimination is used to determine if the question adequately distinguishes between those
students who have the required knowledge to answer the question and those who don't. It is calculated
using the following formula:

\[ \textit{Item Discriminator} = \frac{(\textit{Upper 25\% Correct}) 
                  - (\textit{Lower 25\% Correct})}{\textit{Total Number of Respondents}} \]

\noindent where the upper 25\% of the students are defined to be those students who scored in the top 25\% on the test and got the question correct.
The lower 25\% of students are defined similarly.

Item discrimination can be calculated using the \verb!ItemDiscriminator()! function. It takes as input
the results of the \verb!grades()! function and produces one result per question.  It can be called using the following syntax:

<<echo=true, results=verbatim>>=
ItemDiscriminator(GradedTests())
@

A question with a negative item discrimination value should be reviewed by the instructor, as
the correct answer may be misspecified.

\subsection{Point Biserial Correlation}

Point biserial correlation is also a measure of item discrimination. It is calculated through
the following formula:

\[ \textit{Point Biserial} = \frac{M_{p} - M_{q}}{\sigma\sqrt{pq}} \]

\noindent where:

\begin{itemize}
\item $M_{p}$ is the mean score of students answering the question correctly
\item $M_{q}$ is the mean score of students answering the question incorrectly
\item $\sigma$ is the standard deviation of the exam mean
\item p is the proportion of students answering the question correctly
\item q is the proportion of students answering the question incorrectly
\end{itemize}

Point biserial correlation can be calculated using the \verb!PointBiserial()! function.
It takes as input the result of the \verb!grades()! function, and produces one result 
per question.  It is called using the following syntax:
<<echo=true,results=verbatim>>=
PointBiserial(GradedTests())
@

Point biserial correlation should be used in conjunction with the calculated item discrimination
statistic to determine if a question adequately discriminates between those students who have the
required knowledge and those who don't.

Any question with a point biserial value below 0.2 should be reviewed by the instructor, as
it does not sufficiently distinguish good and bad students.

\subsection{Option Frequency}

Using the \verb!AnswerCounts()! function, a count of how many students
chose each answer option is provided. This function takes as input the
index automatically created by the test creation process (read in
as ``TestIndex.csv"), as well as the result of the
\verb!grades()! function.  Also included are counts of blank responses
and responses that don't correspond to one of the offered choices.

The purpose of the \verb!CreateIndex()! function is to undo the answer
choice randomization that is done during the test creation process, so
that firm counts can be given for each answer choice, not just counts
of how many students chose ``A", ``B", ``C", etc.

The output of this function is a dataframe containing the \verb!AnswerCount! data
rearranged to undo the test randomization.  Columns \verb!A1! to \verb!A5!
contain the counts of students choosing those answers, and columns \verb!R1!
to \verb!R5! give the letter chosen by the randomization.

Below are some of the rows of the output dataframe for our sample test:

<<echo=true,results=verbatim>>=
Index(read.csv(TestIndex.csv, head=TRUE))
Counts <- CreateIndex()
o <- with(Counts, order(Question, ExamCode))
Counts[o[1:6],]
@

Any answer option which has very few respondents should be reviewed by
the instructor. If the answer choice is NA, this means that no such
option was available on the test (e.g. there were only four answer
choices, not five).

\subsection{Distractor Discrimination}

An item discrimination value is also calculated for each of the answer options. This is accomplished
in the exact same way as described in the item discrimination section \ref{sec:ItemDiscrimination}
(where it is done for the question as a whole). Ideally, the only answer option with a positive item discrimination value will be the correct answer. All other options should have a negative item discrimination value, which tells the instructor that the students who scored
lowest on the test are choosing that option more frequently than the students who scored highest on the test.

Any incorrect answer choice with a positive item discrimination should be evaluated by the instructor.

Both the option frequency and distractor discrimination statistics are
combined in a single table for convenience. Table \ref{tab:DistractorDiscrimination}
shows what is
produced for the first question of our sample test.
\begin{table}
\begin{center}
<<>>=
QuestionCounter(1)
Index(read.csv(TestIndex.csv))
Summary <- CreateIndex()

Dis <- DistractorDiscrimination(GradedTests()) 
AnswerCounts <- subset(Summary, Question==QuestionCounter(), 
                       select=paste0("A", 1:nrow(Dis)))
Frequency <- apply(AnswerCounts, 2, sum)
Percentage <- 100*Frequency/nrow(GradedTests())
CountFrame <- data.frame(Dis,Frequency,Percentage)
latex(tabular(Factor(Option)*Heading()*identity 
              ~ Frequency + Percentage + Discrimination,
              data=CountFrame), digits=2)
@
\end{center}
\caption{Distractor discrimination results for the first question on the sample test.
\label{tab:DistractorDiscrimination}}
\end{table}

\subsection{Option Frequency Plot}

The option frequency plot provides a way for the instructor to visualise which answer choices were most popular
amongst the test takers. Frequencies are provided as boxes, and are separated by exam code (if more than one
is given). The boxes of the correct answers are white, while the boxes of incorrect answers are shaded red.

The plot is created by calling the function \verb!answerPlots()!. This function takes as input 
a vector of question numbers (and optionally colour choices).
For example, the following will produce an option frequency plot for the first question
of our sample data:
<<OptionFrequency,eval=false,echo=true>>=
answerPlots(1)
@%$
The resulting plot for the first question is shown in Figure \ref{fig:OptionFrequency}.
\begin{figure}
\begin{center}
<<fig=TRUE>>=
<<OptionFrequency>>
@
\end{center}
\caption{Option frequency plot for the first question on the sample test.\label{fig:OptionFrequency}}
\end{figure}

\subsection{Empirical Probability Plot}

The empirical probability plot displays the estimated probability of
answering the question correctly versus the test scores.

From this graph, we can deduce the empirical probability of a student answering the question correctly
given his or her score on the test.

It would be expected that this graph would be monotonically increasing, from the lowest score achieved on
the test to the highest score.

The plot can be created using the \verb!EmpiricalProbabilityPlot()! function. It takes as input the result
from the \verb!grades()! function, as well as the question number for which the plot will created. A plot
for the first question can be created as follows:
<<Empirical,eval=false,echo=true>>=
EmpiricalProbabilityPlot(GradedTests(), 1)
@
The result is shown in Figure \ref{fig:Empirical}.
\begin{figure}
\begin{center}
<<fig=TRUE>>=
<<Empirical>>
@
\end{center}
\caption{The empirical probability plot for question 1.\label{fig:Empirical}}
\end{figure}

A question in which the plot exhibits a decreasing trend overall as opposed to an increasing trend should
be reviewed by the instructor.

\subsection{Warnings}

Part of the analysis of the multiple choice test is providing the instructor with information on the
appropriateness of the calculated values of the statistics. If the values of the item discrimination, 
difficulty rating, or point biserial statistics fall below standards, 
the built in function \verb!Warnings()! will produce a message informing the instructor,
as well as pointing the instructor to resources with suggestions on improving (or possibly eliminating) 
said question.

Warnings are produced if any combination of the following occur:

\begin{itemize}
\item The item discrimination statistic falls below 0
\item The difficulty rating falls below 0.2
\item The point biserial correlation falls below 0.2
\end{itemize}

The \verb!Warnings()! function takes the calculated values of the item discrimination, difficulty
rating, and point biserial correlation as inputs. The syntax is as follows:
<<echo=true>>=

  DR <- DifficultyRating(GradedTests(), 1)
  ID <- ItemDiscriminator(GradedTests(),1)
  PB <- PointBiserial(GradedTests(), 1)

  Warnings(DR, ID, PB)
@
[No warnings are printed]

A number of different outputs can be produced depending on which statistics are at unacceptable
levels.  For example:

<<echo=true>>=
  DR <- DifficultyRating(GradedTests(), 12)
  ID <- ItemDiscriminator(GradedTests(),12)
  PB <- PointBiserial(GradedTests(), 12)

  Warnings(DR, ID, PB)
@
If there are two statistics that are below acceptable standards:
<<echo=true>>=
DR <- 0.01

Warnings(DR, ID, PB)
@
If all three statistics are below acceptable standards:
<<echo=true>>=
ID <- -0.8

Warnings(DR, ID, PB)
@

Warnings are also given on the Statistical Overview page if the KR20 statistic falls below 0.7 or Ferguson's Delta falls below 0.9.

\section{Report Creation}
\label{sec:report}

The report creation process was designed to be very simple. There are two separate methods for the instructor
to choose from - the full report, and the analysis only report. Both are discussed below.

\subsection{Full Report}

The full report can be used by those instructors who have created their entire test using the provided 
Sweave template. The creation of the test itself is discussed in Section \ref{sec:preparing}. 

By simply modifying a few global variables in the template, the instructor can create a full report
to analyze the results of the test.

The template provides a number of variables that are under the user's control.  For example:

\begin{verbatim}
set.seed(18)
library(Sweavetest)
Initialization("sample", "Student")
testversion(4)

\end{verbatim}

To create the report, the user needs to change the following:
\begin{itemize}
\item Set the \verb!Version()! to ``Report" rather than ``Student''
\item Specify the Scantron file they wish to use.
\end{itemize}

The variables not mentioned above should not be changed or modified in any way, unless the instructor is familiar with \LaTeX\ and wishes to modify the preset formatting done by the templates.

From here, everything else is achieved automatically through the built in functions.
The instructor need only process the newly modified .Rnw file, and a pdf report will be created.
The report will include reproduction of the original questions, with answers given in un-randomized
order, as well as all of the analysis described in Section \ref{sec:analysis}.

An appendix is also provided at the end of the report, using the function \verb!Appendix()!. This function takes no inputs and is called auatomatically when a report is ordered by the instructor. The appendix contains brief explanations of all the statistics used throughout the report. This will allow the instructor to quickly and efficiently
look up the definition of any of the statistics, as well as suggestions on how to improve
the question if the calculated statistics were found to be below acceptable standards.

References are also provided if the instructor wants more information on any of the statistics
in the report.

\subsection{Analysis Only}

If the instructor did not use the Sweave template to create their test, analysis is still possible
using the \pkg{Sweavetest} package. 

If the user only has the results of the test, they should use the low level template provided with
the package. This template produces many of the same statistics as the full report, but it will not 
be able to reproduce the questions and answer options, nor will it be able to undo any randomization
in the answer options.

This method does however, provide the instructor with a function that will allow the report to recognize the correspondence between randomization in questions, as opposed to the randomization in answer options that is done during the full report creation process. The function that must be called to determine these correspondences is the \verb!Unscramble()! function.

The \verb!Unscramble()! function takes the following as input:

\begin{itemize}
\item The result of the \verb!readScanex()! function
\item A vector of exam codes
\item A matrix of question orders, with rows corresponding to the order the exam codes were entered
\item A variable called Scrambled, which takes on TRUE if there is randomization in the question order, and FALSE otherwise
\end{itemize}

The syntax for using this function is as follows:

\begin{verbatim}
scanex <- readScanex(''sample.dat'')
ExamCodes <- c(170,738,840,967)
Master <- c(1:30)
Order1 <- c(15:30, 1:14)
Order2 <- c(20:30,1:10, 15:19,11:14)
Order3 <- c(30:1)
Orders <- data.frame("170"=Master,"738"=Order1,"840"=Order2,"967"=Order3, check.names=FALSE)

NewScanex <- Unscramble(scanex,Orders)

\end{verbatim}

We can see below what the function does by comparing the original Scantron file to the new scantron file:

<<>>=
ExamCodes <- c(170,738,840,967)
Master <- c(1:30)
Order1 <- c(15:30, 1:14)
Order2 <- c(20:30,1:10, 15:19,11:14)
Order3 <- c(30:1)
Orders <- data.frame("170"=Master,"738"=Order1,"840"=Order2,"967"=Order3, check.names=FALSE)

NewScanex <- Unscramble(scanex,Orders)
@

First 6 student answers of old scanex

<<results=verbatim>>=
scanex$Answers[5:10]
@

First 6 student answers of new scanex

<<results=verbatim>>=
NewScanex$Answers[5:10]
@

We can see that the answers have been re-ordered in the new Scantron file as specified by the
instructor. This will allow the \verb!LowLevelReport()! function to create a report with the proper
correspondences.

Note: The \verb!Unscramble()! function is not used when running a full report, as question randomization is not done.

The provided Sweave template provides several variables that are under the users control:

\begin{verbatim}
set.seed(18)
library(Sweavetest)
library(tables)
setwd("C:/Users/Sample")
scanex <- readScanex("sample.dat")
\end{verbatim}

Most of these variables can be left untouched. The user must only make three modifications
to the variables above:

\begin{itemize}
\item The working directory must be changed to the location of the Scantron file that will be read in
\item The file in the \verb!readScanex()! function must be changed to the desired Scantron file
\item If the user has chosen to put the questions in different orders on each version of the exam, the \verb!Unscramble()! function must be used
\end{itemize}

No other variables should be modified, unless the instructor is familiar with \LaTeX\ formatting and 
wishes to change the pre-set formatting done by the template.

At this point the instructor need only process the Sweave document, and a report will be created
in the working directory.

\subsection{Report Without Using A Scantron}

If the instructor uses grading technology other than that provided by Scantron, they may still use
the analysis in the \pkg{Sweavetest} package, but must modify their input file to meet the specifications of 
the functions.

The instructor can create a .dat file with columns matching those produced by the \texttt{readScanex()} function;
see Table \ref{tab:scantron}.

An example file is given below. For best results, the users file should mirror the contents of this file as closely as possible, including column titles.

<<results=verbatim>>=
scanex[5:10,]
@

One important aspect of the Scantron file is the answer keys. They are denoted by Student ID 999999999, and have the same columns as the students. The answer keys for our sample test are provided below:

<<results=verbatim>>=
scanex[1:4,]
@

Note that the answer keys do not require a section number, and should not count in the total exam count.

Once this file has been created, the instructor can carry on with the report process as described above (with either the full report or using the \verb!LowLevelReport()! function).

\section{Merging Class Lists}
\label{sec:Merge}

The final tool provided by the \pkg{Sweavetest} package allows the instructor to export the results of the \verb!grades()! function and appends it to a pre-existing class list. This is done by calling the \verb!MergeClassList()! function.

This function takes as input the class list read in using the \verb!read.csv! function, to which we are appending the results of \verb!grades()!. A sample class list has been provided with this package, and the class list used should mirror its format. A small snapshot is provided below:

<<>>=
Classlist <- system.file("sample/ClassListTemplate.csv", package = "Sweavetest")
SampleClassList <- read.csv(Classlist)
@

<<results=verbatim>>=
head(SampleClassList)
@

Note that the column titles have been slightly modified by R. In the CSV file, the first column is entitled \verb!"Student #"!. All other periods in the title names correspond to blank spaces.

A vector of class sections as characters, as well as a title for the test are also required as input. The original Scantron file must also be passed as input

There are a couple optional variables under the instructors control. If the instructor has decided to exclude some of the questions from the exam (based on the analysis provided in the report), the \verb!weights! variable can be changed to a vector of 0s and 1s, corresponding to each question on the test. The user can change the name of the file to be exported from its default value of "MasterClassList", and the instructor can also request that one file be exported concatenating all class sections by changing the \verb!AcceptSectionCSV! variable to TRUE.

The following syntax should be used when calling the \verb!MergeClassList()! function. In our example, we have decided to exclude the first question from the weighting:

\begin{verbatim}

classlist <- read.csv("MasterClassList.csv")
Sections <- c("001", "002")
TestTitle <- "Test 1"
weights <- c(0, rep(1, 29))

MergeClassList(classlist, scanex, weights, Sections, TestTitle,
              MasterName = "MasterClassList", AcceptMasterCSV = TRUE)

\end{verbatim}

This will append the results of the \verb!grades()! function to the classlist that was specified.

This function uses another function, the \verb!MergeLists()! function, to actually append  the two files. The main purpose of the \verb!MergeClassList()! function is to format the file so that it is clean when it is exported back to the user.

Note that the \verb!MergeClassList()! function is not called automatically at any point during the report creation process. This function must be called separately by instructor if they wish to merge the results into a pre-existing class list.

\section{Cheating Detection}
\label{sec:cheating}

\begin{figure}
\begin{center}
<<fig=TRUE>>=
x <- 0:7
y <- 1:8
df <- expand.grid(x=x,y=y)
df$version <- with(df, 2*(y %% 2) + (x %% 2) + 1)
par(mar=c(0,0,0,0))
plot(y ~ x, pch = as.character(version), data=df, axes=FALSE,
      xlab=NULL, ylab=NULL, cex=1.2)
@
\caption{Seating pattern for four test versions to avoid having a student sit next
to another student with the same version. \label{fig:seating}}
\end{center}
\end{figure}

A common form of cheating on multiple choice tests is simply to copy the answers from a 
neighbour's or confederate's paper.  \pkg{Sweavetest} produces multiple versions of the tests
in order to make copying more difficult; with four test versions distributed in the pattern shown
in Figure \ref{fig:seating}, no student sits next to another student with the same version of the test.
The randomization functions described in section \ref{sec:questions} are designed so that
randomized answers in up to four different versions of the test end up different;  thus 
a student who copies from another student who has the correct answer will likely get the question
wrong.

To detect this kind of cheating, the \verb!wrongKey()! function re-marks each test using
all the other answer keys.  If a student has a higher score when marked with a different
answer key, it's a sign of either copying from a neighbour, or miscoding the answer key.


\section{Future Directions}

\pkg{Sweavetest} was originally developed by the first author (Murdoch) to support
his own teaching needs; only in 2012 was an attempt made to make it more generally useful,
so that others could use it.  There are still several changes planned to support this
direction.  
\begin{itemize}
\item The randomization of multiple choice items is designed for running four versions of the
tests.  Each version will have a different correct answer (unless the answer is one
of the unrandomized ones, such as ``none of the above'').  However, this scheme allows
a clever cheater to learn something by seeing what his/her neighbours have written, and
some instructors would prefer independent randomizations.

\item Not all instructors are comfortable with Sweave and \LaTeX.  Support for \pkg{odfWeave}
\citep{pkg:odfWeave}
or other schemes for test preparation will be considered.

\item We do not currently assign seating for students.  While we do record seating arrangements,
the records are on paper and are not normally digitized.   If these records were digitized, then
the cheating detection could look for correspondences between neighbouring students,
rather than between a student and the answer key for a different version of the test.
Even without those records, it could look for unusual correspondents between all pairs of 
students, albeit with reduced detection power.

\end{itemize}

\section*{Acknowledgments}

This research was supported by an NSERC Discovery Grant to the first author, and an
NSERC Undergraduate Summer Research Award to the second author.

\section{Not covered yet}

Update this list as we cover things.

Still need to document

 [1]          "answerMatrix"        "clean"             
 [6]          "grades"                              
[16] "nominalRoll"        "perms"                "readScanex"           


\bibliography{Sweavetest}

\end{document}

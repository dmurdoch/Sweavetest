Appendix <- function(){
  if(Version() == "Report" | Version() == "Low Level"){
    cat("\\markright{}")
    cat("\\begin{center}")
    cat("\\large\\bf Appendix")
    cat("\\end{center}")
    cat("\\ \\\\")
    cat("\\ \\\\")
    cat("\\underline{Difficulty Rating}")
    cat("\\ \\\\")
    cat("\\ \\\\")
    cat("Difficulty Rating$^{[1][5][7]}$ measures the number of students who answered the question correctly. A question is considered to be too difficult if less than 20\\% of students answer it correctly. A question that has been identified as being too difficult should be checked for improper coding by the instructor. The instructor should also check the answer distribution graph for the question to check for the most popular distractor. If one distractor has been chosen overwhelmingly by the students, the instructor should ensure that the distractor was not plausible given the wording of the question. The instructor should also check for ambiguity in the question, and should consider removing the question if they believe the question was too difficult.")
    cat("\\ \\\\")
    cat("\\ \\\\")
    cat("\\underline{Item Discriminator}")
    cat("\\ \\\\")
    cat("\\ \\\\")
    cat("Item Discrimination$^{[1][5][7]}$ can be calculated by ranking the students according to total score and then selecting the top 25\\% and the lowest 25\\% of students in terms of total score. For each item, the percentage of students in the upper and lower groups answering the question correctly is calculated, and the difference between these percentages yields the item discrimination index. A negative item discrimination value is unacceptable, as this would imply that the bottom 25\\% of students answered the question correctly more often than the top 25\\% of students. The instructor should review an item that has a negative item discriminator, as the question may be miskeyed, ambiguous, or misleading")
    cat("\\ \\\\")
    cat("\\ \\\\")
    cat("\\underline{Point Biserial Correlation}")
    cat("\\ \\\\")
    cat("\\ \\\\")
    cat("The Point Biserial Correlation$^{[1][2][7]}$ is another measure of item discrimination - it measure again how well the top students perform as compared to the bottom students. As with any correlation, point biserial correlation may take on values between -1 and 1, with positive values indicating that the top students performed better than the weaker students on the question. As a general rule, values above .2 are desired. If the point biserial correlation of a question is below .2, the instructor should review the question, as it may be miskeyed, ambiguous, or misleading.")
    cat("\\ \\\\")
    cat("\\newpage")
    cat("\\underline{Kuder-Richardson 20}")
    cat("\\ \\\\")
    cat("\\ \\\\")
    cat("The Kuder-Richardson 20$^{[3][4][5][7]}$ statistic is a measure of test reliability. Specifically, KR20 measures the internal consistency of the test, which is to say it measures how consistently the items on the test provide information about the knowledge of the students on the tested subject. It is expected that students with a high knowledge base in the tested area will answer most of the items correctly, while students with a low knowledge base will answer many of the questions incorrectly. The typical range of the KR20 statistic is between 0 and 1 (while KR20 can be negative in theory, it is extremely unlikely that this will occur). Current industry standards state that the KR20 statistic is at an acceptable level if it is above .7. A value above .7 identifies the test as being reliable, and that the test does a sufficient job in distinguishing between the students who have a strong knowledge base in the tested subject and those who don't. If the KR20 statistic falls below .7, the instructor should review the questions with low difficulty, item discrimination, and point biserial correlation to improve the value of the KR20 statistic and the overall reliability of the test.")
    cat("\\ \\\\")
    cat("\\ \\\\")
    cat("\\underline{Ferguson's Delta}")
    cat("\\ \\\\")
    cat("\\ \\\\")
    cat("Ferguson's Delta$^{[1]}$ is also a measure of test reliability, but instead of measuring internal reliability, it measures how broadly the total scores are distributed over the possible range. A typical normal distribution will yield a value of Ferguson's Delta of .93. As such, the accepted industry standard is a value greater than .9. If the calculated value of Ferguson's Delta is less than .9, the instructor should review the questions with low difficulty, item discrimination, and point biserial correlation to improve the value of Ferguson's Delta and the overall reliability of the test.")
    cat("\\ \\\\")
    cat("\\ \\\\")
    cat("\\underline{Answer Correlation Plot}")
    cat("\\ \\\\")
    cat("\\ \\\\")
    cat("The answer correlation plot is used to help identify possible bad questions. It plots the difficulty of the question along the x-axis, and the correlation between student responses and student scores on the y-axis. That is to say, a question in which the majority of the correct answers come from students scoring high grades on the test will have a higher correlation than a question on which  the majority of correct answers come from student scoring poorly on the test. Particularly easy questions (those questions with x-axis values near 1), will tend to have correlation values near 0, as most students are answering them correctly. A difficult question with a low correlation value should be examined as a potentially problematic question (i.e. questions that occur in the bottom left corner of the graph). The instructor should use this graph in conjunction with the statistics and polots provided for the individual question to decide whether a question is potentially problematic.")
    cat("\\newpage")
    cat("\\begin{center}")
    cat("\\large\\bf References")
    cat("\\end{center}")
    cat("\\ \\\\")
    cat("\\ \\\\")
    cat("1. R. Beichner and L. Ding, Approaches to Data Analysis of Multiple-Choice \\\\ Questions, Physics Education Research 5, 020103 (2009).")
    cat("\\ \\\\")
    cat("\\ \\\\")
    cat("2. J.D. Brown, Point Biserial Correlation Coefficients, JLT Testing and Evlution SIG \\\\ Newsletter. 5 (3) October 2001 (pp. 13 - 17); <http://jalt.org/test/PDF/Brown12.pdf>")
    cat("\\ \\\\")
    cat("\\ \\\\")
    cat("3. R. Dennick and M. Tavakol, Making Sense of Cronbach's Alpha, International \\\\ Journal of Medical Education. 2011; 2:53-55")
    cat("\\ \\\\")
    cat("\\ \\\\")
    cat("4. M.J. Miller, Reliability and Validity, RES 600: Graduate Research Methods; \\\\ <http://michaeljmillerphd.com/res500\\_lecturenotes/reliability\\_and\\_validity.pdf>")
    cat("\\ \\\\")
    cat("\\ \\\\")
    cat("5. J. Patock, Exam Scores: How to Interpret your Statistical Analysis Reports. \\\\ Arizona State Testing Services (2004) <http://www.asu.edu/uts/pdf/InterpIAS.pdf>")
    cat("\\ \\\\")
    cat("\\ \\\\")
    cat("6. D. Rizopoulosm, Package 'ltm', (2011); \\\\ <http://rwiki.sciviews.org/doku.php?id=packages:cran:ltm>")
    cat("\\ \\\\")
    cat("\\ \\\\")
    cat("7. S. Tucker, Using Remark Statistics for Test Reliability and Item Analysis, \\\\ University of Maryland School of Pharmacy, (2007).")
    cat("\\ \\\\")
    
  }
}